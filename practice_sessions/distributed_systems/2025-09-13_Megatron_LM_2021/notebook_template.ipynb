{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d769a465",
   "metadata": {},
   "source": [
    "# Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\n",
    "\n",
    "**Focus**: 3. Performance Analysis of Parallelization Configurations\n",
    "\n",
    "**References**: \n",
    "- Efficient Large-Scale Language Model Training on GPU Clusters\n",
    "Using Megatron-LM: https://arxiv.org/pdf/2104.04473\n",
    "- Nvidia/Docs/Using NCCL (Nvidia Collective Communications Library)/Operations: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html#reducescatter\n",
    "\n",
    "**Purpose**: \n",
    "\n",
    "**Approach**: \n",
    "\n",
    "**Result**: \n",
    "\n",
    "\n",
    "**Definitions**: \n",
    "\n",
    "**Notes**:\n",
    "\n",
    "Pre-requisites to complete the communication analysis\n",
    "NCCL (Nvidia Collective Communications Library) Operations:\n",
    "\n",
    "1. All-Reduce: reductions on data (ex. sum) across devices and writes the result int he receive buffers of *every* rank\n",
    "\n",
    "2. Broadcast: copies N-element buffer on the root rank to all ranks\n",
    "\n",
    "3. Reduce: same operation as All-Reduce, but writes the result in the receive buffer of the specified root rank\n",
    "- All-Reduce = Reduce -> Broadcast\n",
    "\n",
    "4. All-Gather: each K processors aggregates N (sharded) values for every processor into an output dimension K*N\n",
    "- *Ring* All-Reduce (bandwidth optimal) = Reduce Scatter -> All Gather, executed over a logical topological ring\n",
    "\n",
    "5. Reduce-Scatter: Reduce, but output is sharded col-wise across ranks.\n",
    "\n",
    "\n",
    "3.2 Tensor and Pipeline Model Parallelism\n",
    "- The key idea is that TP communication cost per microbatch = l^{stage} * (8bsh((t-1)/t))\n",
    "    - Derivation:\n",
    "        - Ring All-Reduce = Reduce Scatter + All-Gather\n",
    "        - Reduce Scatter = t-1 chunks are sent out and received per GPU, where chunk size = (N/t). Therefore, the data communication cost per GPU = (t-1) * (N/t)\n",
    "        - All-Gather = same data cost as above\n",
    "        - Ring All-Reduce Cost = 2 * (t-1) * (N/t)\n",
    "        - Forward Pass = 1 Attn + 1 FF Ring All-Reduce Operations = 4 * (t-1) * (N/t)\n",
    "        - Forward + Backward Pass = 8 * (t-1) * (N/t) = 8N*((t-1)/t) per device per layer\n",
    "- Therefore, the takeaway is that we should apply TP size = all GPUs in a server/node, and then PP size across nodes to reduce the communication cost.\n",
    "\n",
    "**FAQs**:\n",
    "\n",
    "**Action items**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d6f6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
