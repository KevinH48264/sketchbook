{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b231f60c",
   "metadata": {},
   "source": [
    "# Megatron-LM Code\n",
    "\n",
    "**Focus**: Going through implementation snippets, mainly reading and trying to understand the code rather than running it\n",
    "- Specifically, I will be looking at 2019 Megatron-LM's core contribution: tensor model parallelism and try to trace exactly how Megatron-LM implements tensor parallelism.\n",
    "- Ex. How does Megatron-LM split a Linear layer across multiple GPUs and still produce the correct output?\n",
    "\n",
    "**References**: \n",
    "- https://github.com/NVIDIA/Megatron-LM\n",
    "\n",
    "**Purpose**: to understand the Megatron-LM implementation of tensor model parallelism\n",
    "\n",
    "**Approach**: I'll go into its open-source github repo and try to trace through the core bits of the code.\n",
    "\n",
    "*Definitions*: \n",
    "\n",
    "*Notes*:\n",
    "\n",
    "```plaintext\n",
    "Megatron-LM/\n",
    "├── megatron/                    \n",
    "│   ├── core/                    # Megatron Core (kernels, parallelism, building blocks)\n",
    "│   │   ├── models/              # Transformer models\n",
    "│   │   ├── transformer/         # Transformer building blocks\n",
    "│   │   ├── tensor_parallel/     # Tensor parallelism\n",
    "```\n",
    "\n",
    "It looks like layers.py = examples on how to run parallelism for Linear Layers and then mappings.py is great for how to implement all-reduce and all-gather.\n",
    "\n",
    "```plaintext\n",
    "https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/__init__.py\n",
    "\n",
    "__all__ = [\n",
    "    # cross_entropy.py\n",
    "    \"vocab_parallel_cross_entropy\",\n",
    "    # data.py\n",
    "    \"broadcast_data\",\n",
    "    # layers.py\n",
    "    \"ColumnParallelLinear\",\n",
    "    \"RowParallelLinear\",\n",
    "    \"VocabParallelEmbedding\",\n",
    "    \"set_tensor_model_parallel_attributes\",\n",
    "    \"set_defaults_if_not_set_tensor_model_parallel_attributes\",\n",
    "    \"copy_tensor_model_parallel_attributes\",\n",
    "    \"param_is_not_tensor_parallel_duplicate\",\n",
    "    \"linear_with_grad_accumulation_and_async_allreduce\",\n",
    "    # mappings.py\n",
    "    \"copy_to_tensor_model_parallel_region\",\n",
    "    \"gather_from_tensor_model_parallel_region\",\n",
    "    \"gather_from_sequence_parallel_region\",\n",
    "    \"reduce_from_tensor_model_parallel_region\",\n",
    "    \"reduce_scatter_to_sequence_parallel_region\",\n",
    "    \"scatter_to_tensor_model_parallel_region\",\n",
    "    \"scatter_to_sequence_parallel_region\",\n",
    "    # random.py\n",
    "    \"checkpoint\",\n",
    "    \"get_cuda_rng_tracker\",\n",
    "    \"model_parallel_cuda_manual_seed\",\n",
    "    \"get_expert_parallel_rng_tracker_name\",\n",
    "    \"CheckpointWithoutOutput\",\n",
    "    # utils.py\n",
    "    \"split_tensor_along_last_dim\",\n",
    "    \"split_tensor_into_1d_equal_chunks\",\n",
    "    \"gather_split_1d_tensor\",\n",
    "]\n",
    "```\n",
    "\n",
    "It looks like there are 2 classes that are important here\n",
    "\n",
    "1. class ColumnParallelLinear\n",
    "- Linear layer with column parallelism. \n",
    "- Q: What's column parallelism? A: If our linear layer is Y = XA + b. A is parallelized along its second dimension (column) as A = [A_1, ..., A_p].\n",
    "- Note: column parallel usually requires all-gathers if you want to get the full result\n",
    "- So it looks like the implementation is largely the same? Probably the weight splitting happens in `output_parallel = self._forward_impl(`, which for a normal forward pass with gradient computation happens in `linear_with_grad_accumulation_and_async_allreduce`. \n",
    "    - Edit: looks like the weight sharding might happen at init?\n",
    "    - Specifically in the initialize weight section: \n",
    "\n",
    "\n",
    "2. class RowParallelLinear\n",
    "- In general, it looks like when you run this tensor_parallel/layers.py RowParallelLinear, each node will get the world_size, and the identify the self.input_size_per_partition which is the sharding. Then weight variables are created which are the sharded parts. \n",
    "- During the forward pass, basically linear_with_grad_accumulation_and_async_allreduce() is called, which completes a tensor parallel all reduce (for row-wise sharding) by calling LinearWithGradAccumulationAndAsyncCommunication?\n",
    "\n",
    "Oh neat, \"LinearWithGradAccumulationAndAsyncCommunication\" contains the forward AND backward.\n",
    "- Forward funciton does indeed save the input and weight for backward\n",
    "- Fusing allows you to defer weight grad GEMM (matmul) for better efficiency\n",
    "\n",
    "reduce-scatter?\n",
    "- reduce scatter just means that you compute the hcunk you'll keep (so it's just a sharded reduce basically)\n",
    "\n",
    "sequence parallel - you can shard activations along the sequence dimension somewhat surprisingly across ranks. This helps for long sequence training.\n",
    "\n",
    "**Result**: \n",
    "- These are all tricks to avoid running out of GPU RAM, because memory is often (and still is!) a bottleneck. Tensor parallelism + Sequence Parallel are techniques employed in Megatron for large scale machine learning.\n",
    "\n",
    "\n",
    "**FAQs**:\n",
    "\n",
    "**Action items**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8edf0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key code lines for ColumnParallelLinear\n",
    "# typically you set TP size via --tensor-model-parallel-size. Otherwise it defaults to 1. \n",
    "\n",
    "self.tp_group = get_tensor_model_parallel_group_if_none(\n",
    "    self.tp_group, is_expert=self.is_expert\n",
    ")\n",
    "\n",
    "# def get_pg_size(group=None):\n",
    "#     \"\"\"Get world size for a distributed group.\n",
    "\n",
    "#     Args:\n",
    "#         group: Process group to get world size for. If None, uses default group.\n",
    "\n",
    "#     Returns:\n",
    "#         int: World size (1 if distributed not initialized or group is None, else group.size())\n",
    "#     \"\"\"\n",
    "#     if not torch.distributed.is_initialized() or group is None:\n",
    "#         return 1\n",
    "#     return group.size()\n",
    "world_size = get_pg_size(self.tp_group)\n",
    "\n",
    "# We divide the output_size by the world_size (tensor parallel group size)\n",
    "# def ensure_divisibility(numerator, denominator):\n",
    "#     \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n",
    "#     assert numerator % denominator == 0, \"{} is not divisible by {}\".format(numerator, denominator)\n",
    "# def divide(numerator, denominator):\n",
    "#     \"\"\"Ensure that numerator is divisible by the denominator and return\n",
    "#     the division value.\"\"\"\n",
    "#     ensure_divisibility(numerator, denominator)\n",
    "#     return numerator // denominator\n",
    "self.output_size_per_partition = divide(output_size, world_size)\n",
    "\n",
    "# Create the parameter which should has the tensors stored as (out_features, in_features)\n",
    "self.weight = Parameter(\n",
    "    torch.empty(\n",
    "        self.output_size_per_partition, self.input_size, dtype=config.params_dtype\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initialize affine weight for model parallel on GPU.\n",
    "_initialize_affine_weight_gpu(\n",
    "    self.weight,\n",
    "    init_method,\n",
    "    partition_dim=0,\n",
    "    stride=stride,\n",
    "    is_expert=self.is_expert,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
