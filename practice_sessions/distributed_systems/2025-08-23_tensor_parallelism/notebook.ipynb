{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49fd574",
   "metadata": {},
   "source": [
    "# Tensor Parallelism\n",
    "References:\n",
    "- Demystifying Tensor Parallelism (https://robotchinwag.com/posts/demystifying-tensor-parallelism/#pairwise-sharding)\n",
    "\n",
    "Purpose: expand beyond 1 GPU model training and support training larger models\n",
    "\n",
    "Approach: leverage 3D parallelism, specifically tensor parallelism here, to split layer weights across devices\n",
    "\n",
    "Result: enables increased memory capacity and higher throughput, but there's a large communication cost for all-gather and all-reduce operations\n",
    "\n",
    "Definitions:\n",
    "- pairwise sharding = each device (or rank) is assigned a *pair* of weight shards, i.e. Pair = $(A_i, B_i)$ and operations with them can happen on the device (no collective operation needed until the end). Usually, $A_i$ is column sharded, and $B_i$ is row-sharded and then an all-reduce operation is needed. In the backward, because of the transposes, it's flipped sharding that ends in an all-reduce so the backward gradients are also pairwise sharding\n",
    "- forward, row-shard = need all-reduce (sum) at the end to combine partial outputs\n",
    "- backward w.r.t. X, col-shard = need all-gather at the end to combine partial derivatives\n",
    "\n",
    "Notes:\n",
    "- V1.1 - it looks like we use all-reduce whenever we need the full result as input to something like an activation function ReLU. Otherwise, we use an all-gather?\n",
    "- V1.2 - actually because ReLU is a pointwise operation, we can avoid an extra collective operation of AllReduce after the first matmul and instead focus on doing a col-wise weight sharding s.t. we just do a row-wise weight sharding in the second mat mul followed by a AllReduce. This allows us to avoid 1 collective operation which is important to reduce networking bottlenecks.\n",
    "There is a pattern of column wise sharding -> rowwise sharding = \"pairwise sharding\" to reduce the need for an All-Gather. This can be applied to the Attention layer in a Transformer model as well (Q: how?)\n",
    "\n",
    "\n",
    "Gradients of tensor parallel layer\n",
    "- the backwards of a pairwise sharded layer is also pairwise sharded, because we operate in reverse order and the sharding is flipped because of transposes\n",
    "- a forward col-wise sharded matmul (which uses all-gather) = row-wise sharded in backprop to use all-reduce to sum partial gradients\n",
    "- Q: Why transpose A in backprop computation in matmuls? \n",
    "    - A: Y=XA, X'=Y'@A^T. This is because each row j in A tells us how much X dimension j contributes to each output dimension in Y. Therefore, when we transpose, A^T, row j because col j, and given the gradients for each dimension in Y, we can compute how much a change in dimension j in X will be, based on the dimension gradients in Y and the each contribution listed in A^T col j.\n",
    "\n",
    "Action items:\n",
    "- COMPLETE - Q: we know that to compute gradients w.r.t X is flipped sharding because while the forward Y = XA, X' = Y'@A^T, but how about for the actual weights gradients?\n",
    "Well actually, no collective operations are needed to udpate the weights - each rank/node just updates its own shard weights locally with its local gradient.\n",
    "\n",
    "- TODO - Q: How to improve the pairwise sharding implementation so it's a bit more comprehensive?\n",
    "Primarily completed it, it should be pretty good conceptually. Only next step is just really actually using torch.distributed.all_reduce instead of a built in self.reduce().\n",
    "\n",
    "- TODO - Q: How can we apply pairwise sharding in the attention layer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise sharding - col-wise weight sharding -> row-wise weight sharding -> all-reduce\n",
    "# Assumes 2-way tensor parallel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, D, expansion=4):\n",
    "        super().__init__()\n",
    "        hidden = expansion * D\n",
    "        self.fc1_W_DH = torch.randn(D, hidden)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2_W_HD = torch.randn(hidden, D)\n",
    "\n",
    "    def all_reduce(self, parts: List[torch.Tensor]):\n",
    "        # assume we have some all reduce collective operation\n",
    "        # simulate sum across tensor-parallel ranks\n",
    "        # in real code: out = Z2_BND_local; torch.distributed.all_reduce(out, op=SUM) # synchronous/blocking collective - waits for all ranks to reach that call and the reduction is finished\n",
    "        return parts[0] + parts[1]\n",
    "\n",
    "    def all_gather(self, parts: List[torch.Tensor]):\n",
    "        # assume we have some all gather collective operation\n",
    "        return torch.cat([parts[0], parts[1]], dim=1)\n",
    "\n",
    "    def forward(self, X_BND):\n",
    "        Z1_BNH = X_BND @ self.fc1_W_DH # X = XA\n",
    "        H_BNH = self.act(Z1_BNH) # X = ReLU(X)\n",
    "        Z2_BND = H_BNH @ self.fc2_W_HD # Y = XB\n",
    "        return Z2_BND\n",
    "    \n",
    "    def forward_pairwise_sharded(self, X_BND):\n",
    "        \"\"\"\n",
    "        weight col-wise sharding -> ReLU -> weight row-wise sharding -> all-reduce\n",
    "        which is more optimal than:\n",
    "        weight row-wise sharding -> all-reduce -> ReLU -> weight col-wise sharding -> all-gather\n",
    "        \"\"\"\n",
    "        # tracking shapes\n",
    "        B, N, D = X_BND.shape\n",
    "        _, H = self.fc1_W_DH.shape\n",
    "        K = D // 2\n",
    "        L = H // 2\n",
    "\n",
    "        # weight col-wise sharding\n",
    "        fc1_W_DL_0, fc1_W_DL_1 = self.fc1_W_DH[:, :L], self.fc1_W_DH[:, L:]\n",
    "\n",
    "        Z1_BNL_0 = X_BND @ fc1_W_DL_0\n",
    "        Z1_BNL_1 = X_BND @ fc1_W_DL_1\n",
    "\n",
    "        # ReLU\n",
    "        H_BNL_0 = self.act(Z1_BNL_0)\n",
    "        H_BNL_1 = self.act(Z1_BNL_1)\n",
    "\n",
    "        # weight row-wise sharding\n",
    "        fc2_W_LD_0, fc2_W_LD_1 = self.fc2_W_HD[:L, :], self.fc2_W_HD[L:, :]\n",
    "        Z2_BND_0 = H_BNL_0 @ fc2_W_LD_0\n",
    "        Z2_BND_1 = H_BNL_1 @ fc2_W_LD_1\n",
    "\n",
    "        # all-reduce\n",
    "        Z2_BND = self.all_reduce([Z2_BND_0, Z2_BND_1])\n",
    "\n",
    "        return Z2_BND\n",
    "        \n",
    "\n",
    "B, N, D = 32, 128, 768\n",
    "ff = FeedForward(D=D)\n",
    "X_BND = torch.randn(B, N, D) # (B,N,D)\n",
    "Y_BND = ff(X_BND)\n",
    "Y_BND_pairwise = ff.forward_pairwise_sharded(X_BND)\n",
    "assert Y_BND.shape == (B, N, D)\n",
    "assert Y_BND.shape == Y_BND_pairwise.shape\n",
    "assert torch.allclose(Y_BND, Y_BND_pairwise, rtol=1e-2, atol=1e-4) # FP32 has slight rounding differences so we increase the tolerance here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddbb5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise sharding - col-wise weight sharding -> row-wise weight sharding -> all-reduce\n",
    "# Assumes 2-way tensor parallel\n",
    "# Adding a backward function now, without biases for simplicity\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, D, expansion=4):\n",
    "        super().__init__()\n",
    "        hidden = expansion * D\n",
    "        self.fc1_W_DH = torch.randn(D, hidden)\n",
    "        self.act = nn.ReLU() # using ReLU for easier derivation\n",
    "        self.fc2_W_HD = torch.randn(hidden, D)\n",
    "\n",
    "        self.cache = None\n",
    "        self.grads = {}\n",
    "\n",
    "    def all_reduce(self, parts: List[torch.Tensor]):\n",
    "        # assume we have some all reduce collective operation\n",
    "        # simulate sum across tensor-parallel ranks\n",
    "        # in real code: out = Z2_BND_local; torch.distributed.all_reduce(out, op=SUM) # synchronous/blocking collective - waits for all ranks to reach that call and the reduction is finished\n",
    "        return sum(parts)\n",
    "\n",
    "    def all_gather(self, parts: List[torch.Tensor]):\n",
    "        # assume we have some all gather collective operation\n",
    "        return torch.cat([parts[0], parts[1]], dim=1)\n",
    "\n",
    "    def forward(self, X_BND):\n",
    "        Z1_BNH = X_BND @ self.fc1_W_DH # X = XA\n",
    "        H_BNH = self.act(Z1_BNH) # X = ReLU(X)\n",
    "        Z2_BND = H_BNH @ self.fc2_W_HD # Y = XB\n",
    "\n",
    "        self.cache = (X_BND, Z1_BNH, H_BNH)\n",
    "        return Z2_BND\n",
    "    \n",
    "    def backward(self, dY_BND):\n",
    "        \"\"\"\n",
    "        Assume we have a dY_BND = \\delta_{loss} / \\delta_{Y_BND} which is computed from the derivative of the differentiable loss function\n",
    "        \"\"\"\n",
    "        X_BND, Z1_BNH, H1_BNH = self.cache\n",
    "\n",
    "        dW2_HD = torch.einsum(\"bnd,bnh->hd\", dY_BND, H1_BNH)\n",
    "        dH1_BNH = torch.einsum(\"bnd,hd->bnh\", dY_BND, self.fc2_W_HD)\n",
    "\n",
    "        # derivative of ReLU\n",
    "        dZ1_BNH = dH1_BNH * (Z1_BNH > 0).to(dH1_BNH.dtype)\n",
    "\n",
    "        dW1_DH = torch.einsum(\"bnh,bnd->dh\", dZ1_BNH, X_BND)\n",
    "        dX_BND = torch.einsum(\"bnh,dh->bnd\", dZ1_BNH, self.fc1_W_DH)\n",
    "\n",
    "        self.grads = {\n",
    "            \"dW1_DH\": dW1_DH,\n",
    "            \"dW2_HD\": dW2_HD,\n",
    "        }\n",
    "        \n",
    "        return dX_BND\n",
    "\n",
    "    def forward_pairwise_sharded(self, X_BND):\n",
    "        \"\"\"\n",
    "        weight col-wise sharding -> ReLU -> weight row-wise sharding -> all-reduce\n",
    "        which is more optimal than:\n",
    "        weight row-wise sharding -> all-reduce -> ReLU -> weight col-wise sharding -> all-gather\n",
    "        \"\"\"\n",
    "        # tracking shapes\n",
    "        B, N, D = X_BND.shape\n",
    "        _, H = self.fc1_W_DH.shape\n",
    "        assert H % 2 == 0, \"H must be divisble by 2\"\n",
    "        K = D // 2\n",
    "        L = H // 2\n",
    "\n",
    "        # weight col-wise sharding\n",
    "        fc1_W_DL_0, fc1_W_DL_1 = self.fc1_W_DH[:, :L], self.fc1_W_DH[:, L:]\n",
    "\n",
    "        Z1_BNL_0 = X_BND @ fc1_W_DL_0\n",
    "        Z1_BNL_1 = X_BND @ fc1_W_DL_1\n",
    "\n",
    "        # ReLU\n",
    "        H_BNL_0 = self.act(Z1_BNL_0)\n",
    "        H_BNL_1 = self.act(Z1_BNL_1)\n",
    "\n",
    "        # weight row-wise sharding\n",
    "        fc2_W_LD_0, fc2_W_LD_1 = self.fc2_W_HD[:L, :], self.fc2_W_HD[L:, :]\n",
    "        Z2_BND_0 = H_BNL_0 @ fc2_W_LD_0\n",
    "        Z2_BND_1 = H_BNL_1 @ fc2_W_LD_1\n",
    "\n",
    "        # all-reduce\n",
    "        Z2_BND = self.all_reduce([Z2_BND_0, Z2_BND_1])\n",
    "\n",
    "        self.cache = X_BND, Z1_BNL_0, Z1_BNL_1, H_BNL_0, H_BNL_1\n",
    "\n",
    "        return Z2_BND\n",
    "    \n",
    "    def backward_pairwise_sharded(self, dY_BND):\n",
    "        \"\"\"\n",
    "        Assume we have a dY_BND = \\delta_{loss} / \\delta_{Y_BND} which is computed from the derivative of the differentiable loss function\n",
    "\n",
    "        Backward gradients:\n",
    "        dL/dX = dL/dY (B_0^T @ ReLU'(F) @ A_0^T + B_1^T @ ReLU'(G) @ A_1^T)\n",
    "        - final operation is an all-reduce because we're summing gradients\n",
    "\n",
    "        where the pairwise sharded forward pass is:\n",
    "\n",
    "        X_BND -> F_BNL = X_0_BND@A_0_DL -> M_BNL = ReLU(F_BNL) -> K_BND = M_BNL@B_0_LD -> Y_BND = K_BND + H_BND\n",
    "             |-> G_BNL = X_1_BND@A_1_DL -> N_BNL = ReLU(G_BNL) -> H_BND = N_BNL@B_1_LD ->|\n",
    "\n",
    "        Where dim_L = H // 2, A = fc1_W_DH and B = fc2_W_HD\n",
    "        A_DH is split col-wise into A_0_DL, A_1_DL\n",
    "        B_HD is split row-wise into B_0_LD, B_1_LD\n",
    "        \"\"\"\n",
    "        X_BND, F_BNL, G_BNL, M_BNL, N_BNL = self.cache\n",
    "\n",
    "        # define dimension shapes\n",
    "        B, N, D = dY_BND.shape\n",
    "        _, H = self.fc1_W_DH.shape\n",
    "        assert H % 2 == 0, \"H must be divisble by 2\"\n",
    "        L = H // 2\n",
    "\n",
    "        # define the split weight shards\n",
    "        A_DH = self.fc1_W_DH\n",
    "        B_HD = self.fc2_W_HD\n",
    "\n",
    "        A_0_DL, A_1_DL = A_DH[:, :L], A_DH[:, L:]\n",
    "        B_0_LD, B_1_LD = B_HD[:L, :], B_HD[L:, :]\n",
    "\n",
    "        # compute gradients for second linear pass per shard\n",
    "        dB_0_LD = torch.einsum(\"bnd,bnl->ld\", dY_BND, M_BNL)\n",
    "        dM_BNL = torch.einsum(\"bnd,ld->bnl\", dY_BND, B_0_LD)\n",
    "\n",
    "        dB_1_LD = torch.einsum(\"bnd,bnl->ld\", dY_BND, N_BNL)\n",
    "        dN_BNL = torch.einsum(\"bnd,ld->bnl\", dY_BND, B_1_LD)\n",
    "\n",
    "        # compute gradients for ReLU\n",
    "        dF_BNL = dM_BNL * (F_BNL > 0).to(dM_BNL.dtype)\n",
    "        dG_BNL = dN_BNL * (G_BNL > 0).to(dN_BNL.dtype)\n",
    "\n",
    "        # compute gradients for first linear pass per shard\n",
    "        dA_0_DL = torch.einsum(\"bnl,bnd->dl\", dF_BNL, X_BND)\n",
    "        dX_0_BND = torch.einsum(\"bnl,dl->bnd\", dF_BNL, A_0_DL)\n",
    "\n",
    "        dA_1_DL = torch.einsum(\"bnl,bnd->dl\", dG_BNL, X_BND)\n",
    "        dX_1_BND = torch.einsum(\"bnl,dl->bnd\", dG_BNL, A_1_DL)\n",
    "\n",
    "        # all reduce dX_BND\n",
    "        dX_BND = self.all_reduce([dX_0_BND, dX_1_BND])\n",
    "\n",
    "        self.grads = {\n",
    "            # gradients for shards on device 0\n",
    "            \"dB_0_LD\": dB_0_LD,\n",
    "            \"dA_0_DL\": dA_0_DL,\n",
    "\n",
    "            # gradients for shards on device 1\n",
    "            \"dB_1_LD\": dB_1_LD,\n",
    "            \"dA_1_DL\": dA_1_DL,\n",
    "        }\n",
    "\n",
    "        return dX_BND\n",
    "        \n",
    "\n",
    "B, N, D = 32, 128, 768\n",
    "ff = FeedForward(D=D)\n",
    "\n",
    "# test forward pass + forward pairwise sharded are equal\n",
    "X_BND = torch.randn(B, N, D) # (B,N,D)\n",
    "Y_BND = ff(X_BND)\n",
    "Y_BND_pairwise = ff.forward_pairwise_sharded(X_BND)\n",
    "assert Y_BND.shape == (B, N, D)\n",
    "assert Y_BND.shape == Y_BND_pairwise.shape\n",
    "assert torch.allclose(Y_BND, Y_BND_pairwise, rtol=1e-2, atol=1e-4) # FP32 has slight rounding differences so we increase the tolerance here\n",
    "\n",
    "# test backward pass and backward pairwise sharded are equal\n",
    "dY_BND = torch.randn(B, N, D) # random values to just check compiling and shapes for now\n",
    "Y_BND = ff(X_BND)\n",
    "dX_BND = ff.backward(dY_BND)\n",
    "\n",
    "Y_BND_pairwise = ff.forward_pairwise_sharded(X_BND)\n",
    "dX_BND_pairwise = ff.backward_pairwise_sharded(dY_BND)\n",
    "\n",
    "assert dX_BND.shape == (B, N, D)\n",
    "assert dX_BND_pairwise.shape == (B, N, D)\n",
    "assert torch.allclose(dX_BND, dX_BND_pairwise, rtol=1e-2, atol=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
