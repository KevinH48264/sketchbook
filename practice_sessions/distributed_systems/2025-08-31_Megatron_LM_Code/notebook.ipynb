{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b231f60c",
   "metadata": {},
   "source": [
    "# Megatron-LM Code\n",
    "\n",
    "**Focus**: Going through implementation snippets, mainly reading and trying to understand the code rather than running it\n",
    "- Specifically, I will be looking at 2019 Megatron-LM's core contribution: tensor model parallelism and try to trace exactly how Megatron-LM implements tensor parallelism.\n",
    "- Ex. How does Megatron-LM split a Linear layer across multiple GPUs and still produce the correct output?\n",
    "\n",
    "**References**: \n",
    "- https://github.com/NVIDIA/Megatron-LM\n",
    "\n",
    "**Purpose**: to understand the Megatron-LM implementation of tensor model parallelism\n",
    "\n",
    "**Approach**: I'll go into its open-source github repo and try to trace through the core bits of the code.\n",
    "\n",
    "*Definitions*: \n",
    "\n",
    "*Notes*:\n",
    "\n",
    "```plaintext\n",
    "Megatron-LM/\n",
    "├── megatron/                    \n",
    "│   ├── core/                    # Megatron Core (kernels, parallelism, building blocks)\n",
    "│   │   ├── models/              # Transformer models\n",
    "│   │   ├── transformer/         # Transformer building blocks\n",
    "│   │   ├── tensor_parallel/     # Tensor parallelism\n",
    "```\n",
    "\n",
    "It looks like layers.py = examples on how to run parallelism for Linear Layers and then mappings.py is great for how to implement all-reduce and all-gather.\n",
    "\n",
    "```plaintext\n",
    "https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/__init__.py\n",
    "\n",
    "__all__ = [\n",
    "    # cross_entropy.py\n",
    "    \"vocab_parallel_cross_entropy\",\n",
    "    # data.py\n",
    "    \"broadcast_data\",\n",
    "    # layers.py\n",
    "    \"ColumnParallelLinear\",\n",
    "    \"RowParallelLinear\",\n",
    "    \"VocabParallelEmbedding\",\n",
    "    \"set_tensor_model_parallel_attributes\",\n",
    "    \"set_defaults_if_not_set_tensor_model_parallel_attributes\",\n",
    "    \"copy_tensor_model_parallel_attributes\",\n",
    "    \"param_is_not_tensor_parallel_duplicate\",\n",
    "    \"linear_with_grad_accumulation_and_async_allreduce\",\n",
    "    # mappings.py\n",
    "    \"copy_to_tensor_model_parallel_region\",\n",
    "    \"gather_from_tensor_model_parallel_region\",\n",
    "    \"gather_from_sequence_parallel_region\",\n",
    "    \"reduce_from_tensor_model_parallel_region\",\n",
    "    \"reduce_scatter_to_sequence_parallel_region\",\n",
    "    \"scatter_to_tensor_model_parallel_region\",\n",
    "    \"scatter_to_sequence_parallel_region\",\n",
    "    # random.py\n",
    "    \"checkpoint\",\n",
    "    \"get_cuda_rng_tracker\",\n",
    "    \"model_parallel_cuda_manual_seed\",\n",
    "    \"get_expert_parallel_rng_tracker_name\",\n",
    "    \"CheckpointWithoutOutput\",\n",
    "    # utils.py\n",
    "    \"split_tensor_along_last_dim\",\n",
    "    \"split_tensor_into_1d_equal_chunks\",\n",
    "    \"gather_split_1d_tensor\",\n",
    "]\n",
    "```\n",
    "\n",
    "It looks like there are 2 classes that are important here\n",
    "\n",
    "1. class ColumnParallelLinear\n",
    "- Linear layer with column parallelism. \n",
    "- Q: What's column parallelism? A: If our linear layer is Y = XA + b. A is parallelized along its second dimension (column) as A = [A_1, ..., A_p].\n",
    "- Note: column parallel usually requires all-gathers if you want to get the full result\n",
    "\n",
    "2. class RowParallelLinear\n",
    "\n",
    "\n",
    "**Result**: \n",
    "\n",
    "\n",
    "\n",
    "**FAQs**:\n",
    "- Q: How do I even begin to dive into this codebase?\n",
    "    - A: 1) I've usually found success in finding the train script / the core script to run and going deep from there? or 2) I need to do a DFS and go in with a very narrow starting point that connects directly to the 2019 paper. The main contribution is likely TENSOR MODEL PARALLELISM. \n",
    "\n",
    "**Action items**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b357d2f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
