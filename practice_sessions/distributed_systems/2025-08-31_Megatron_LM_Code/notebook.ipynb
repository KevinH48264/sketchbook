{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b231f60c",
   "metadata": {},
   "source": [
    "# Megatron-LM Code\n",
    "\n",
    "**Focus**: Going through implementation snippets, mainly reading and trying to understand the code rather than running it\n",
    "- Specifically, I will be looking at 2019 Megatron-LM's core contribution: tensor model parallelism and try to trace exactly how Megatron-LM implements tensor parallelism.\n",
    "- Ex. How does Megatron-LM split a Linear layer across multiple GPUs and still produce the correct output?\n",
    "\n",
    "**References**: \n",
    "- https://github.com/NVIDIA/Megatron-LM\n",
    "\n",
    "**Purpose**: to understand the Megatron-LM implementation of tensor model parallelism\n",
    "\n",
    "**Approach**: I'll go into its open-source github repo and try to trace through the core bits of the code.\n",
    "\n",
    "*Definitions*: \n",
    "\n",
    "*Notes*:\n",
    "\n",
    "```plaintext\n",
    "Megatron-LM/\n",
    "├── megatron/                    \n",
    "│   ├── core/                    # Megatron Core (kernels, parallelism, building blocks)\n",
    "│   │   ├── models/              # Transformer models\n",
    "│   │   ├── transformer/         # Transformer building blocks\n",
    "│   │   ├── tensor_parallel/     # Tensor parallelism\n",
    "```\n",
    "\n",
    "It looks like layers.py = examples on how to run parallelism for Linear Layers and then mappings.py is great for how to implement all-reduce and all-gather.\n",
    "\n",
    "```plaintext\n",
    "https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/__init__.py\n",
    "\n",
    "__all__ = [\n",
    "    # cross_entropy.py\n",
    "    \"vocab_parallel_cross_entropy\",\n",
    "    # data.py\n",
    "    \"broadcast_data\",\n",
    "    # layers.py\n",
    "    \"ColumnParallelLinear\",\n",
    "    \"RowParallelLinear\",\n",
    "    \"VocabParallelEmbedding\",\n",
    "    \"set_tensor_model_parallel_attributes\",\n",
    "    \"set_defaults_if_not_set_tensor_model_parallel_attributes\",\n",
    "    \"copy_tensor_model_parallel_attributes\",\n",
    "    \"param_is_not_tensor_parallel_duplicate\",\n",
    "    \"linear_with_grad_accumulation_and_async_allreduce\",\n",
    "    # mappings.py\n",
    "    \"copy_to_tensor_model_parallel_region\",\n",
    "    \"gather_from_tensor_model_parallel_region\",\n",
    "    \"gather_from_sequence_parallel_region\",\n",
    "    \"reduce_from_tensor_model_parallel_region\",\n",
    "    \"reduce_scatter_to_sequence_parallel_region\",\n",
    "    \"scatter_to_tensor_model_parallel_region\",\n",
    "    \"scatter_to_sequence_parallel_region\",\n",
    "    # random.py\n",
    "    \"checkpoint\",\n",
    "    \"get_cuda_rng_tracker\",\n",
    "    \"model_parallel_cuda_manual_seed\",\n",
    "    \"get_expert_parallel_rng_tracker_name\",\n",
    "    \"CheckpointWithoutOutput\",\n",
    "    # utils.py\n",
    "    \"split_tensor_along_last_dim\",\n",
    "    \"split_tensor_into_1d_equal_chunks\",\n",
    "    \"gather_split_1d_tensor\",\n",
    "]\n",
    "```\n",
    "\n",
    "It looks like there are 2 classes that are important here\n",
    "\n",
    "1. class ColumnParallelLinear\n",
    "- Linear layer with column parallelism. \n",
    "- Q: What's column parallelism? A: If our linear layer is Y = XA + b. A is parallelized along its second dimension (column) as A = [A_1, ..., A_p].\n",
    "- Note: column parallel usually requires all-gathers if you want to get the full result\n",
    "- So it looks like the implementation is largely the same? Probably the weight splitting happens in `output_parallel = self._forward_impl(`, which for a normal forward pass with gradient computation happens in `linear_with_grad_accumulation_and_async_allreduce`. \n",
    "    - Edit: looks like the weight sharding might happen at init?\n",
    "    - Specifically in the initialize weight section: \n",
    "\n",
    "2. class RowParallelLinear\n",
    "\n",
    "\n",
    "**Result**: \n",
    "\n",
    "\n",
    "\n",
    "**FAQs**:\n",
    "- Q: How do I even begin to dive into this codebase?\n",
    "    - A: 1) I've usually found success in finding the train script / the core script to run and going deep from there? or 2) I need to do a DFS and go in with a very narrow starting point that connects directly to the 2019 paper. The main contribution is likely TENSOR MODEL PARALLELISM. \n",
    "\n",
    "**Action items**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8edf0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key code lines for ColumnParallelLinear\n",
    "# typically you set TP size via --tensor-model-parallel-size. Otherwise it defaults to 1. \n",
    "\n",
    "self.tp_group = get_tensor_model_parallel_group_if_none(\n",
    "    self.tp_group, is_expert=self.is_expert\n",
    ")\n",
    "\n",
    "# def get_pg_size(group=None):\n",
    "#     \"\"\"Get world size for a distributed group.\n",
    "\n",
    "#     Args:\n",
    "#         group: Process group to get world size for. If None, uses default group.\n",
    "\n",
    "#     Returns:\n",
    "#         int: World size (1 if distributed not initialized or group is None, else group.size())\n",
    "#     \"\"\"\n",
    "#     if not torch.distributed.is_initialized() or group is None:\n",
    "#         return 1\n",
    "#     return group.size()\n",
    "world_size = get_pg_size(self.tp_group)\n",
    "\n",
    "# We divide the output_size by the world_size (tensor parallel group size)\n",
    "# def ensure_divisibility(numerator, denominator):\n",
    "#     \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n",
    "#     assert numerator % denominator == 0, \"{} is not divisible by {}\".format(numerator, denominator)\n",
    "# def divide(numerator, denominator):\n",
    "#     \"\"\"Ensure that numerator is divisible by the denominator and return\n",
    "#     the division value.\"\"\"\n",
    "#     ensure_divisibility(numerator, denominator)\n",
    "#     return numerator // denominator\n",
    "self.output_size_per_partition = divide(output_size, world_size)\n",
    "\n",
    "# Create the parameter which should has the tensors stored as (out_features, in_features)\n",
    "self.weight = Parameter(\n",
    "    torch.empty(\n",
    "        self.output_size_per_partition, self.input_size, dtype=config.params_dtype\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initialize affine weight for model parallel on GPU.\n",
    "_initialize_affine_weight_gpu(\n",
    "    self.weight,\n",
    "    init_method,\n",
    "    partition_dim=0,\n",
    "    stride=stride,\n",
    "    is_expert=self.is_expert,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
