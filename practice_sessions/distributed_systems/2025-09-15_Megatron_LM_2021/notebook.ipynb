{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d769a465",
   "metadata": {},
   "source": [
    "# Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\n",
    "\n",
    "**Focus**: Continue reading from \"3.3 Data and Model Parallelism\"\n",
    "\n",
    "**References**: \n",
    "- Efficient Large-Scale Language Model Training on GPU Clusters\n",
    "Using Megatron-LM: https://arxiv.org/pdf/2104.04473\n",
    "- Nvidia/Docs/Using NCCL (Nvidia Collective Communications Library)/Operations: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html#reducescatter\n",
    "\n",
    "**Purpose**: \n",
    "\n",
    "**Approach**: \n",
    "\n",
    "**Result**: \n",
    "\n",
    "\n",
    "**Definitions**: \n",
    "\n",
    "**Notes**:\n",
    "\n",
    "Consider the interaction between data parallelism and the two types of model parallelism\n",
    "\n",
    "3.3.1 Pipeline Model parallelism\n",
    "<div>\n",
    "    <img src=\"2025-09-15_DP_vs_Pipeline_Bubble_Size.png\" width=\"30%\">\n",
    "</div>\n",
    "\n",
    "- In Figure 6, we do see that \n",
    "    - for n=32, increasing the total # of microbatches (b') from 32 to 128 does decrease the pipeline bubble size from 1.0 to 0.25. \n",
    "    - We also see that increasing DP will also help reduce the pipeline bubble size, and fundamentally this is because $p = \\frac{n}{d}$, so if d increases, then p decreases until 1, where pipeline size = 1 and there are no pipeline bubbles at all.\n",
    "    - The main barrier to actually increasing $d$ is when the model itself is too large for 1 GPU, so PP > 1.\n",
    "\n",
    "**FAQs**:\n",
    "- Where did this come from: \"communicaiton time for a ring-based implementation scales with $\\frac{d-1}{d}$\n",
    "\n",
    "**Action items**:\n",
    "- Continue from FAQ 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d6f6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
