{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1f923d",
   "metadata": {},
   "source": [
    "# Dense Layer from Scratch\n",
    "Forward + backward pass as basics\n",
    "\n",
    "Notes:\n",
    "- TODO: conv2D from scratch, and then attention layer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "20 mins to implement\n",
    "\n",
    "Questions:\n",
    "- What to do if training diverges?\n",
    "1. the learning rate might be too high, test lower learning rates\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, D, expansion=4):\n",
    "        super().__init__()\n",
    "\n",
    "        H = D * expansion\n",
    "        self.W1_DH = torch.randn(D, H)\n",
    "        self.b1_H = torch.randn(H)\n",
    "\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.W2_HD = torch.randn(H, D)\n",
    "        self.b2_D = torch.randn(D)\n",
    "\n",
    "        self.cache = None\n",
    "        self.grads = {}\n",
    "\n",
    "    def forward(self, X_BND):\n",
    "        \"\"\"\n",
    "        Time and Memory Complexity Analysis:\n",
    "            Assumptions:\n",
    "            1. expansion=1 so h = d\n",
    "\n",
    "            Total FLOPs: O(bndh) = O(bnd^2)\n",
    "            Memory: O(bnd + d^2)\n",
    "\n",
    "            ratio of memory to FLOPs = O(1/d + 1/bn)\n",
    "            So if we want to decrease the ratio (because usually we have more FLOPs per memory bytes), we should increase d or bn\n",
    "        \"\"\"\n",
    "\n",
    "        Z1_BNH = torch.einsum(\"bnd,dh->bnh\", X_BND, self.W1_DH) + self.b1_H\n",
    "        H1_BNH = self.act1(Z1_BNH)\n",
    "        Y_BND = torch.einsum(\"bnh,hd->bnd\", H1_BNH, self.W2_HD) + self.b2_D\n",
    "\n",
    "        self.cache = (X_BND, Z1_BNH, H1_BNH)\n",
    "        return Y_BND\n",
    "\n",
    "    def backward(self, dY_BND):\n",
    "        \"\"\"\n",
    "        Assume we have a dY_BND = \\delta_{loss} / \\delta_{Y_BND} which is computed from the derivative of the differentiable loss function\n",
    "        \"\"\"\n",
    "        X_BND, Z1_BNH, H1_BNH = self.cache\n",
    "\n",
    "        dW2_HD = torch.einsum(\"bnd,bnh->hd\", dY_BND, H1_BNH)\n",
    "        db2_D = torch.einsum(\"bnd->d\", dY_BND)\n",
    "        dH1_BNH = torch.einsum(\"bnd,hd->bnh\", dY_BND, self.W2_HD)\n",
    "\n",
    "        # derivative of ReLU\n",
    "        dZ1_BNH = dH1_BNH * (Z1_BNH > 0).to(dH1_BNH.dtype)\n",
    "\n",
    "        dW1_DH = torch.einsum(\"bnh,bnd->dh\", dZ1_BNH, X_BND)\n",
    "        db1_H = torch.einsum(\"bnh->h\", dZ1_BNH)\n",
    "        dX_BND = torch.einsum(\"bnh,dh->bnd\", dZ1_BNH, self.W1_DH)\n",
    "\n",
    "        self.grads = {\n",
    "            \"dW1_DH\": dW1_DH,\n",
    "            \"db1_H\": db1_H,\n",
    "            \"dW2_HD\": dW2_HD,\n",
    "            \"db2_D\": db2_D,\n",
    "        }\n",
    "        \n",
    "        return dX_BND\n",
    "\n",
    "    def step(self, lr=1e-3):\n",
    "        # update parameters to reduce the loss\n",
    "        with torch.no_grad():\n",
    "            self.W1_DH -= self.grads[\"dW1_DH\"] * lr\n",
    "            self.b1_H -= self.grads[\"db1_H\"] * lr\n",
    "            self.W2_HD -= self.grads[\"dW2_HD\"] * lr\n",
    "            self.b2_D -= self.grads[\"db2_D\"] * lr\n",
    "\n",
    "    def _test_forward_shapes(self):\n",
    "        B, N, D = 2, 3, 4\n",
    "        X_BND = torch.randn(B, N, D)\n",
    "        Y_BND = self.forward(X_BND)\n",
    "        assert Y_BND.shape == (B, N, D)\n",
    "\n",
    "    def _test_forward_values(self):\n",
    "        B, N, D = 2, 3, 4\n",
    "        X_BND = torch.zeros(B, N, D)\n",
    "\n",
    "        # set biases to zero\n",
    "        self.b1_H.zero_()\n",
    "        self.b2_D.zero_()\n",
    "\n",
    "        Y_BND = self.forward(X_BND)\n",
    "\n",
    "        assert torch.allclose(Y_BND, torch.zeros_like(Y_BND))\n",
    "\n",
    "    def _test_backward_shapes(self):\n",
    "        B, N, D = 2, 3, 4\n",
    "        X_BND = torch.randn(B, N, D)\n",
    "        Y_BND = self.forward(X_BND)\n",
    "        assert Y_BND.shape == (B, N, D)\n",
    "\n",
    "        # assume a basic MSE\n",
    "        target = torch.randn(B, N, D)\n",
    "        mse_loss = ((Y_BND - target) ** 2).mean()\n",
    "        dloss = 2 * (Y_BND - target) / (B * N * D)\n",
    "        dY_BND = dloss\n",
    "\n",
    "        dX_BND = self.backward(dY_BND)\n",
    "        assert dX_BND.shape == (B, N, D)\n",
    "\n",
    "    def _test_backward_values(self):\n",
    "        B, N, D = 2, 3, 4\n",
    "        X_BND = torch.zeros(B, N, D)\n",
    "\n",
    "        # set biases to zero\n",
    "        self.b1_H.zero_()\n",
    "        self.b2_D.zero_()\n",
    "\n",
    "        Y_BND = self.forward(X_BND)\n",
    "\n",
    "        assert Y_BND.shape == (B, N, D)\n",
    "        assert torch.allclose(Y_BND, torch.zeros_like(Y_BND))\n",
    "\n",
    "        # assume a basic MSE\n",
    "        target = torch.zeros(B, N, D)\n",
    "        mse_loss = ((Y_BND - target) ** 2).mean()\n",
    "        dloss = 2 * (Y_BND - target) / (B * N * D)\n",
    "        dY_BND = dloss\n",
    "\n",
    "        dX_BND = self.backward(dY_BND)\n",
    "        assert dX_BND.shape == (B, N, D)\n",
    "        assert torch.allclose(dX_BND, torch.zeros_like(dX_BND))\n",
    "\n",
    "    def _test_step(self):\n",
    "        B, N, D = 2, 3, 4\n",
    "        X_BND = torch.zeros(B, N, D)\n",
    "\n",
    "        # set biases to zero\n",
    "        self.b1_H.zero_()\n",
    "        self.b2_D.zero_()\n",
    "\n",
    "        Y_BND = self.forward(X_BND)\n",
    "\n",
    "        assert Y_BND.shape == (B, N, D)\n",
    "        assert torch.allclose(Y_BND, torch.zeros_like(Y_BND))\n",
    "\n",
    "        # assume a basic MSE\n",
    "        target = torch.zeros(B, N, D)\n",
    "        mse_loss = ((Y_BND - target) ** 2).mean()\n",
    "        dloss = 2 * (Y_BND - target) / (B * N * D)\n",
    "        dY_BND = dloss\n",
    "\n",
    "        dX_BND = self.backward(dY_BND)\n",
    "        assert dX_BND.shape == (B, N, D)\n",
    "        assert torch.allclose(dX_BND, torch.zeros_like(dX_BND))\n",
    "\n",
    "        # optimizer step\n",
    "        prev_W1_DH = self.W1_DH.clone()\n",
    "        prev_b1_H = self.b1_H.clone()\n",
    "        prev_W2_HD = self.W2_HD.clone()\n",
    "        prev_b2_D = self.b2_D.clone()\n",
    "        \n",
    "        self.step()\n",
    "        assert torch.allclose(prev_W1_DH, self.W1_DH)\n",
    "        assert torch.allclose(prev_b1_H, self.b1_H)\n",
    "        assert torch.allclose(prev_W2_HD, self.W2_HD)\n",
    "        assert torch.allclose(prev_b2_D, self.b2_D)\n",
    "\n",
    "    \n",
    "ffn = FeedForwardNetwork(D = 4)\n",
    "ffn._test_forward_shapes()\n",
    "ffn._test_forward_values()\n",
    "ffn._test_backward_shapes()\n",
    "ffn._test_backward_values()\n",
    "ffn._test_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db701b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
