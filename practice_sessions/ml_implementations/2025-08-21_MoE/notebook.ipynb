{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ba170e",
   "metadata": {},
   "source": [
    "# Mixture-of-Experts Implementation\n",
    "**References**: \n",
    "- Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (https://arxiv.org/abs/1701.06538)\n",
    "\n",
    "**Purpose**: The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation. where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are signficiant algorithmic and performance challenges.\n",
    "\n",
    "**Approach**: In this work, Shazeer et al. address those challenges and finally realize the promise of conditional comptuation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. \n",
    "\n",
    "They introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example.\n",
    "\n",
    "They apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. \n",
    "\n",
    "They present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. \n",
    "\n",
    "**Result**: On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "*Theory*\n",
    "\n",
    "Conditional computation\n",
    "- Exploiting scale in both training data and model size has been central to the success of deep learning. however, as both model size and number of training examples increase, this leads to roughly quadratic blow-up in training costs. Unfortunately, the advances in computing power and distributed computation fall short of meeting such demand.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"2025-08-21_MoE.png\" alt=\"diagram\" style=\"width:50%;\">\n",
    "</div>\n",
    "\n",
    "- The main problem and motivation is that the ideas are promising in theory, but no work to date has yet demonstrated massive improvements in model capacity, training time, or model quality\n",
    "\n",
    "Sparsely-Gated Mixture-of-Experts Layer\n",
    "- Note there are MANY ways to implement the theory of mixture-of-experts. The biggest question engineering and practicality wise was what the best way to implement the theory of mixture-of-experts was. Seeing the different perspectives of introducing the idea of using multiple MoEs with their own gating networks as parts of a deep model was key.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "Q1: Aren't there computational challenges in splitting the batch sequences by expert then?\n",
    "- A1: Yep, refer to DeepSpeed-MoE, Megatron-LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db1f50",
   "metadata": {},
   "source": [
    "## Self-Attention Decoder Class\n",
    "Just starting off with the Self-Attention Decoder Class previously implemented because I think that can be easily updated to a MoE architecture. Specifically, I think that the MoE architecture just has the FFN class as MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a39fa9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.3.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.9 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/60.9 kB 330.3 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/60.9 kB 440.4 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.9/60.9 kB 405.6 kB/s eta 0:00:00\n",
      "Downloading numpy-2.3.2-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "   ---------------------------------------- 0.0/13.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/13.1 MB 4.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.9/13.1 MB 20.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.3/13.1 MB 34.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.6/13.1 MB 42.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/13.1 MB 45.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.1/13.1 MB 65.1 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a1bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Dimension key:\n",
    "\n",
    "B = batch_size\n",
    "N = sequence length\n",
    "M = memory length (length of sequence being attended to - K/V)\n",
    "D = model dimension\n",
    "H = number of heads\n",
    "K = head dimension\n",
    "\"\"\"\n",
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, D, H, dropout_p):\n",
    "    super().__init__()\n",
    "\n",
    "    self.ln1 = nn.LayerNorm(D) # normalize over the last dimension which is expected to be of size D\n",
    "    self.attn = SelfAttention(D, H, dropout_p)\n",
    "    self.ln2 = nn.LayerNorm(D)\n",
    "    self.ffn = FeedForward(D, dropout_p)\n",
    "\n",
    "  def forward(self, X):\n",
    "    X = X + self.attn(self.ln1(X))\n",
    "    X = X + self.ffn(self.ln2(X))\n",
    "    return X\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, D, dropout_p, expansion=4):\n",
    "    super().__init__()\n",
    "    hidden = expansion * D\n",
    "    self.fc1 = nn.Linear(D, hidden, bias=True) # bias=True to give higher degrees of freedom\n",
    "    self.act = nn.GELU() # Used in BERT and GPT. Smoother than ReLU\n",
    "    self.fc2 = nn.Linear(hidden, D, bias=True)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, X_BND):\n",
    "    Z1_BNH = self.fc1(X_BND) # (B,N,D) @ (D,H) -> (B,N,H)\n",
    "    H_BNH = self.act(Z1_BNH) # (B,N,H)\n",
    "    Z2_BND = self.fc2(H_BNH) # (B,N,H) @ (H,D) -> (B,N,D)\n",
    "    Y_BND = self.dropout(Z2_BND) # (B,N,D)\n",
    "\n",
    "    # cache / weights\n",
    "    # X_BND, Z1_BNH, H_BNH, Z2_BND, dropout_mask\n",
    "\n",
    "\n",
    "    return Y_BND\n",
    "\n",
    "  # TODO: implement backprop\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, D, H, dropout_p):\n",
    "    super().__init__()\n",
    "    assert D % H == 0, \"D must be divisible by H\"\n",
    "\n",
    "    self.Wq = nn.Linear(D, D)\n",
    "    self.Wk = nn.Linear(D, D)\n",
    "    self.Wv = nn.Linear(D, D)\n",
    "    self.Wo = nn.Linear(D, D)\n",
    "    self.H = H\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, X):\n",
    "    B, N, D = X.shape\n",
    "    H = self.H\n",
    "    K = D // H\n",
    "    M = N\n",
    "\n",
    "    Q_BND = self.Wq(X)\n",
    "    K_BMD = self.Wk(X)\n",
    "    V_BMD = self.Wv(X)\n",
    "\n",
    "    # split to heads\n",
    "    Q_BHNK = Q_BND.reshape(B, N, H, K).permute(0, 2, 1, 3)\n",
    "    K_BHMK = K_BMD.reshape(B, M, H, K).permute(0, 2, 1, 3)\n",
    "    V_BHMK = V_BMD.reshape(B, M, H, K).permute(0, 2, 1, 3)\n",
    "\n",
    "    # causal mask\n",
    "    mask = torch.tril(torch.ones(N, M, device=X.device, dtype=torch.float32))\n",
    "\n",
    "    # compute attention scores\n",
    "    logits_BHNM = Q_BHNK @ K_BHMK.transpose(-2, -1) #torch.einsum(\"BND,BMD->BNM\", Q_BND, K_BMD)\n",
    "    logits_BHNM = logits_BHNM / math.sqrt(K)\n",
    "    logits_BHNM = logits_BHNM.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    weights_BHNM = torch.softmax(logits_BHNM, dim=-1)\n",
    "    O_BHNK = weights_BHNM @ V_BHMK # torch.einsum(\"BNM,BMD->BND\", weights, V_BMD)\n",
    "    O_BND = O_BHNK.permute(0, 2, 1, 3).contiguous().reshape(B, N, D)\n",
    "\n",
    "    Y_BND = self.Wo(O_BND)\n",
    "    Y_BND = self.dropout(Y_BND) # for regularization of the final representation\n",
    "    return Y_BND\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "  def __init__(self, D, H, dropout_p, vocab_size, num_layers, max_seq_len):\n",
    "    super().__init__()\n",
    "\n",
    "    self.tok_embd = nn.Embedding(vocab_size, D)\n",
    "    self.pos_embd = nn.Embedding(max_seq_len, D) # GPT paper uses learned positional embeddings. Easier to implement as well\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    self.layers = nn.ModuleList([AttentionBlock(D, H, dropout_p) for _ in range(num_layers)])\n",
    "    self.ln = nn.LayerNorm(D)\n",
    "\n",
    "    self.lm_head = nn.Linear(D, vocab_size, bias=False)\n",
    "    self.lm_head.weight = self.tok_embd.weight # weight tying\n",
    "\n",
    "  def forward(self, input_ids):\n",
    "    \"\"\"\n",
    "    input_ids: (B, N) # input tokens\n",
    "    returns logits: (B, N, vocab_size)\n",
    "    \"\"\"\n",
    "    B, N = input_ids.shape\n",
    "    pos = torch.arange(N, device=input_ids.device).unsqueeze(0) # (1, N)\n",
    "\n",
    "    X = self.tok_embd(input_ids) + self.pos_embd(pos) # (B, N, D)\n",
    "    X = self.dropout(X)\n",
    "\n",
    "    for l in self.layers:\n",
    "      X = l(X)\n",
    "\n",
    "    X = self.ln(X)\n",
    "    logits = self.lm_head(X) # (B, N, vocab_size)\n",
    "\n",
    "    # apply temperature (usually goes in generate() though)\n",
    "    # logits /= temperature # lower temperature makes higher values go up farther (spikier). higher temperature makes it more diffuse\n",
    "\n",
    "    # apply top-k (usually goes in generate() though)\n",
    "    k = 50\n",
    "    values, _ = torch.topk(logits, k) # (B, N, k)\n",
    "    min_topk = values[:, :, -1] # (B, N)\n",
    "    min_topk = min_topk.unsqueeze(-1) # (B, N, 1)\n",
    "    mask = logits < min_topk # (B, N, vocab_size)\n",
    "    logits[mask] = -float(\"inf\") # remove everything except top-k tokens\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "# TESTS\n",
    "def _test_sa():\n",
    "  B, N, D = 2, 4, 16\n",
    "  H = 8\n",
    "  dropout_p = 0.1\n",
    "  X = torch.randn(B, N, D)\n",
    "  sa = SelfAttention(D, H, dropout_p)\n",
    "  Y = sa(X)\n",
    "  assert Y.shape == (B, N, D)\n",
    "\n",
    "def _test_ff():\n",
    "  B, N, D = 2, 4, 16\n",
    "  H = 8\n",
    "  dropout_p = 0.1\n",
    "  X = torch.randn(B, N, D)\n",
    "  ff = FeedForward(D, dropout_p)\n",
    "  Y = ff(X)\n",
    "  assert Y.shape == (B, N, D)\n",
    "\n",
    "def _test_block():\n",
    "  B, N, D = 2, 4, 16\n",
    "  H = 8\n",
    "  dropout_p = 0.1\n",
    "  X = torch.randn(B, N, D)\n",
    "  block = AttentionBlock(D, H, dropout_p)\n",
    "  Y = block(X)\n",
    "  assert Y.shape == (B, N, D)\n",
    "\n",
    "def _test_decoder():\n",
    "  B, N, D = 2, 4, 16\n",
    "  H = 8\n",
    "  dropout_p = 0.1\n",
    "  vocab_size = 500\n",
    "  num_layers = 4\n",
    "  max_seq_len = 128\n",
    "  input_ids = torch.randint(high=vocab_size, size=(B, N))\n",
    "  block = Decoder(D, H, dropout_p, vocab_size, num_layers, max_seq_len)\n",
    "  logits = block(input_ids)\n",
    "  assert logits.shape == (B, N, vocab_size)\n",
    "\n",
    "\n",
    "_test_sa()\n",
    "_test_ff()\n",
    "_test_block()\n",
    "_test_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a8b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
