{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1f923d",
   "metadata": {},
   "source": [
    "# Self Attention Layer\n",
    "Forward + backward pass as basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37d456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention = Softmax(Q@K^T/\\sqrt{d_k})@V\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, D):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Wq_DD = torch.randn(D, D)\n",
    "        self.Wk_DD = torch.randn(D, D)\n",
    "        self.Wv_DD = torch.randn(D, D)\n",
    "\n",
    "        self.cache = None\n",
    "        self.grads = {}\n",
    "\n",
    "\n",
    "    def forward(self, X_BND):\n",
    "        \"\"\"\n",
    "        Minimal self-attention implementation to focus on gradient computation.\n",
    "\n",
    "        Excludes support for a final projection, residual, layernorm, masking, padding, KV cache.\n",
    "        \"\"\"\n",
    "\n",
    "        B, N, D = X_BND.shape\n",
    "        \n",
    "        Q_BND = torch.einsum(\"bnd,de->bne\", X_BND, self.Wq_DD)\n",
    "        K_BND = torch.einsum(\"bnd,de->bne\", X_BND, self.Wk_DD)\n",
    "        V_BND = torch.einsum(\"bnd,de->bne\", X_BND, self.Wv_DD)\n",
    "\n",
    "        logits_BNM = torch.einsum(\"bnd,bmd->bnm\", Q_BND, K_BND)\n",
    "        logits_BNM = logits_BNM / math.sqrt(D)\n",
    "        weights_BNM = torch.softmax(logits_BNM, dim=-1)\n",
    "\n",
    "        Y_BND = torch.einsum(\"bnm,bmd->bnd\", weights_BNM, V_BND)\n",
    "\n",
    "        self.cache = (weights_BNM, V_BND, K_BND, Q_BND, X_BND)\n",
    "\n",
    "        return Y_BND\n",
    "\n",
    "    def backward(self, dY_BND):\n",
    "        \"\"\"\n",
    "        Softmax Derivative Reference: https://medium.com/data-science/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
    "        softmax: R^N -> R^N = vector function\n",
    "        Jacobian matrix = matrix of all first-order partial derivatives\n",
    "        trick: we take the derivative of the log of the output s.t. we can avoid using the quotient rule for derivatives for softmax which is a bit more complex\n",
    "        Assuming s = softmax(z)\n",
    "        d/dz_j log(s_i) = 1/s_i * ds_i/dz_j\n",
    "        ds_i/dz_j = d/dz_j log(s_i) * s_i\n",
    "        log(s_i) = log(e^z_i / \\sum_j e^z_j) = z_i - log(\\sum_j e^z_j)\n",
    "        d/dz_j log(s_i) = dz_i / dz_j - d/dz_j log(\\sum_j e^z_j)\n",
    "                        = I(i == j)   - 1/\\sum_j e^z_j * d/dz_j \\sum_j e^z_j\n",
    "                        = I(i == j)   - 1/\\sum_l e^z_l * e^z_j\n",
    "        d/dz_j log(s_i) = I(i == j)   - s_j\n",
    "        ds_i/dz_j = (I(i == j) - s_j) * s_i = Jacobian matrix of the softmax\n",
    "\n",
    "        Therefore,\n",
    "        dL/dz_j = dL/ds * ds/dz_j\n",
    "                = \\sum_i (dL/ds_i * ds_i/dz_j)\n",
    "                = \\sum_i (dL/ds_i * (I(i == j) - s_j) * s_i)\n",
    "                ... <applying some casework when i = j and when i != j to the sum ...\n",
    "                = dL/ds_j * s_j (1 - s_j) + \\sum_{i != j} - dL/ds_i * s_i * s_j\n",
    "                = s_j (dL/ds_j (1 - s_j) - \\sum_{i != j} dL/ds_i * s_i)\n",
    "                = s_j (dL/ds_j - \\sum_i dL/ds_i * s_i)\n",
    "        dL/dz_j = s_j (dL/ds_j - dL/ds @ s)\n",
    "\n",
    "\n",
    "\n",
    "        Note: for cross-entropy loss where L(y_true, y_pred) = - \\sum_{i=1}^C y_true_i * log(y_pred_i), the math of dL/dz_j works out s.t. dL/dz = s - y (so a clean derivative where other dependencies cancel out) which makes intuitive sense because the loss is only dependent on the y_true class=1 probability going up, so s - y directly affects that probability, so the gradient computation is quite simple. It's also just clean from the choice of log(s_i) in the loss function where the derivative lets s_i cancel out in the derivation.\n",
    "        \"\"\"\n",
    "        weights_BNM, V_BND, K_BND, Q_BND, X_BND = self.cache\n",
    "        B, N, D = dY_BND.shape\n",
    "\n",
    "        dV_BND = torch.einsum(\"bnd,bnm->bmd\", dY_BND, weights_BNM)\n",
    "        dweights_BNM = torch.einsum(\"bnd,bmd->bnm\", dY_BND, V_BND)\n",
    "\n",
    "        # derivative of a softmax is dL/dz_j = s_j (dL/ds_j - dL/ds @ s) where s = softmax(z). z = logits_BNM, s = weights_BNM in this case\n",
    "        dlogits_BNM = weights_BNM * (dweights_BNM - torch.einsum(\"bnm,bnm->bn\", dweights_BNM, weights_BNM).unsqueeze(-1)) # dL/dz = s * (dL/ds - dL/ds @ s) # Jacobian-vector product (JVP) for the softmax function. # short hand: dz = s * (g - dot) where dot = g @ s, g = dL/ds\n",
    "        dlogits_BNM = dlogits_BNM * (1.0 / math.sqrt(D))\n",
    "        dQ_BND = torch.einsum(\"bnm,bmd->bnd\", dlogits_BNM, K_BND)\n",
    "        dK_BND = torch.einsum(\"bnm,bnd->bmd\", dlogits_BNM, Q_BND)\n",
    "\n",
    "        # compute gradients for projections\n",
    "        dWv_DD = torch.einsum(\"bnd,bne->de\", X_BND, dV_BND)\n",
    "        dX_BND_V = torch.einsum(\"de,bne->bnd\", self.Wv_DD, dV_BND)\n",
    "\n",
    "        dWk_DD = torch.einsum(\"bnd,bne->de\", X_BND, dK_BND)\n",
    "        dX_BND_K = torch.einsum(\"de,bne->bnd\", self.Wk_DD, dK_BND)\n",
    "\n",
    "        dWq_DD = torch.einsum(\"bnd,bne->de\", X_BND, dQ_BND)\n",
    "        dX_BND_Q = torch.einsum(\"de,bne->bnd\", self.Wq_DD, dQ_BND)\n",
    "\n",
    "        dX_BND = dX_BND_Q + dX_BND_K + dX_BND_V\n",
    "\n",
    "        self.grads = {\n",
    "            \"dWq_DD\": dWq_DD,\n",
    "            \"dWk_DD\": dWk_DD,\n",
    "            \"dWv_DD\": dWv_DD,\n",
    "        }\n",
    "\n",
    "        return dX_BND\n",
    "\n",
    "    def _test_forward(self):\n",
    "        B, N, D = 2, 3, 4\n",
    "\n",
    "        X_BND = torch.zeros(B, N, D)\n",
    "        Y_BND = self.forward(X_BND)\n",
    "\n",
    "        assert Y_BND.shape == (B, N, D)\n",
    "        assert torch.allclose(X_BND, torch.zeros(B, N, D))\n",
    "\n",
    "    def _test_backward(self):\n",
    "        B, N, D = 2, 3, 4\n",
    "\n",
    "        X_BND = torch.zeros(B, N, D)\n",
    "        Y_BND = self.forward(X_BND)\n",
    "\n",
    "        assert Y_BND.shape == (B, N, D)\n",
    "        assert torch.allclose(X_BND, torch.zeros(B, N, D))\n",
    "\n",
    "        # backward\n",
    "        dY_BND = torch.zeros(B, N, D)\n",
    "        dX_BND = self.backward(dY_BND)\n",
    "\n",
    "        assert dX_BND.shape == (B, N, D)\n",
    "        assert torch.allclose(dX_BND, torch.zeros(B, N, D))\n",
    "\n",
    "\n",
    "B, N, D = 2, 3, 4\n",
    "sa = SelfAttention(D)\n",
    "sa._test_forward()\n",
    "sa._test_backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
