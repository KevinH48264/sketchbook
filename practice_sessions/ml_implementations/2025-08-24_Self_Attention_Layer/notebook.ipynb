{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1f923d",
   "metadata": {},
   "source": [
    "# Self Attention Layer\n",
    "Forward + backward pass as basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention = Softmax(Q@K^T/\\sqrt{d_k})@V\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, D):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Wq_DD = torch.randn(D, D)\n",
    "        self.Wk_DD = torch.randn(D, D)\n",
    "        self.Wv_DD = torch.randn(D, D)\n",
    "\n",
    "        self.cache = None\n",
    "        self.grads = {}\n",
    "\n",
    "\n",
    "    def forward(self, X_BND):\n",
    "        \"\"\"\n",
    "        Minimal self-attention implementation to focus on gradient computation.\n",
    "\n",
    "        Excludes support for a final projection, residual, layernorm, masking, padding, KV cache.\n",
    "        \"\"\"\n",
    "\n",
    "        B, N, D = X_BND.shape\n",
    "        \n",
    "        Q_BND = torch.einsum(\"bnd,de->bne\", X_BND, self.Wq_DD)\n",
    "        K_BND = torch.einsum(\"bnd,de->bne\", X_BND, self.Wk_DD)\n",
    "        V_BND = torch.einsum(\"bnd,de->bne\", X_BND, self.Wv_DD)\n",
    "\n",
    "        logits_BNM = torch.einsum(\"bnd,bmd->bnm\", Q_BND, K_BND)\n",
    "        logits_BNM = logits_BNM / math.sqrt(D)\n",
    "        weights_BNM = torch.softmax(logits_BNM, dim=-1)\n",
    "\n",
    "        Y_BND = torch.einsum(\"bnm,bmd->bnd\", weights_BNM, V_BND)\n",
    "\n",
    "        self.cache = (weights_BNM, V_BND)\n",
    "\n",
    "        return Y_BND\n",
    "\n",
    "    def backward(self, dY_BND):\n",
    "        \"\"\"\n",
    "        Softmax Derivative Reference: https://medium.com/data-science/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
    "        softmax: R^N -> R^N = vector function\n",
    "        Jacobian matrix = matrix of all first-order partial derivatives\n",
    "        trick: we take the derivative of the log of the output s.t. we can avoid using the quotient rule for derivatives for softmax which is a bit more complex\n",
    "        Assuming s = softmax(z)\n",
    "        d/dz_j log(s_i) = 1/s_i * ds_i/dz_j\n",
    "        ds_i/dz_j = d/dz_j log(s_i) * s_i\n",
    "        log(s_i) = log(e^z_i / \\sum_j e^z_j) = z_i - log(\\sum_j e^z_j)\n",
    "        d/dz_j log(s_i) = dz_i / dz_j - d/dz_j log(\\sum_j e^z_j)\n",
    "                        = I(i == j)   - 1/\\sum_j e^z_j * d/dz_j \\sum_j e^z_j\n",
    "                        = I(i == j)   - 1/\\sum_l e^z_l * e^z_j\n",
    "        d/dz_j log(s_i) = I(i == j)   - s_j\n",
    "        ds_i/dz_j = (I(i == j) - s_j) * s_i = Jacobian matrix of the softmax\n",
    "\n",
    "        Note: for cross-entropy loss where L(y_true, y_pred) = - \\sum_{i=1}^C y_true_i * log(y_pred_i), the math of dL/dz_j works out s.t. dL/dz = s - y (so a clean derivative where other dependencies cancel out) which makes intuitive sense because the loss is only dependent on the y_true class=1 probability going up, so s - y directly affects that probability, so the gradient computation is quite simple. It's also just clean from the choice of log(s_i) in the loss function where the derivative lets s_i cancel out in the derivation.\n",
    "        \"\"\"\n",
    "        weights_BNM, V_BND = self.cache\n",
    "\n",
    "        dV_BND = torch.einsum(\"bnd,bnm->bmd\", dY_BND, weights_BNM)\n",
    "        dweights_BNM = torch.einsum(\"bnd,bmd->bnm\", dY_BND, V_BND)\n",
    "\n",
    "        dlogits_BNM = # derivative of a softmax is...?\n",
    "\n",
    "    def _test_forward(self):\n",
    "        B, N, D = 2, 3, 4\n",
    "\n",
    "        X_BND = torch.zeros(B, N, D)\n",
    "        Y_BND = self.forward(X_BND)\n",
    "\n",
    "        assert Y_BND.shape == (B, N, D)\n",
    "        assert torch.allclose(X_BND, torch.zeros(B, N, D))\n",
    "\n",
    "    def _test_backward(self):\n",
    "        pass\n",
    "\n",
    "B, N, D = 2, 3, 4\n",
    "sa = SelfAttention(D)\n",
    "sa._test_forward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
