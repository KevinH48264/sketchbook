{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fefa42",
   "metadata": {},
   "source": [
    "# Adam Optimizer\n",
    "\n",
    "References: Adam: A Method for Stochastic Optimization (https://arxiv.org/pdf/1412.6980)\n",
    "\n",
    "Purpose: \n",
    "\n",
    "Approach: Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments\n",
    "\n",
    "Result: \n",
    "1. Straightforward to implement, computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. Appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.\n",
    "2. Hyper-parameters have intuitive interpretations and typically require little tuning\n",
    "3. There is a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework\n",
    "4. Empirical results demonstrate Adam works well in practice and compares favorably to other stochastic optimization methods\n",
    "\n",
    "Definitions: \n",
    "\n",
    "Notes:\n",
    "- Many problems in fields of science and engineering can be cast as the optimization of some scalar parameterized objective function requiring maximization or minimization w.r.t its parameters. \n",
    "\n",
    "FAQs:\n",
    "\n",
    "Action items:\n",
    "- I actually didn't get to spend too much time diving into this yet beyond the Introduction, but also found additional papers to look into and likely implement: https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf6e3e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
