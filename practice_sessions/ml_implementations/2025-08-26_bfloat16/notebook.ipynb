{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f320c9df",
   "metadata": {},
   "source": [
    "# bfloat16\n",
    "\n",
    "References: BFloat16: The secret to high performance on Cloud TPUs (2019) (https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)\n",
    "\n",
    "Purpose: to achieve high performance on TPU/GPU devices (higher theoretical FLOPs / device)\n",
    "\n",
    "Approach: A custom floating point format called \"Brain Floating Point Format\" or \"bfloat16\". Used within systolic arrays to accelerate matmuls. \n",
    "\n",
    "Each multiply-accumulate operation in matmul uses bfloat16 for multiplication and 32-bit IEEE floating point (FP32 - standard single-precision floating point) for accumulation. \n",
    "\n",
    "Result: higher performance, model portability, and better numerical stability for deep learning workloads\n",
    "\n",
    "Definitions: \n",
    "\n",
    "Notes:\n",
    "- bloat16 = custom 16-bit floating point format with one sign bit, eight exponent bits, and seven mantissa bits. \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"2025-08-26_bfloat16.png\" style=\"width:75%\">\n",
    "</div>\n",
    "\n",
    "Therefore, bfloat16 has a greater dynamic range (number of exponent bits) than FP16 = dynamic range of FP32. This allows training with bfloat16 to work as well as FP32 format while delivering increased performance and reducing memory usage.\n",
    "\n",
    "Seems to me that bfloat16 is just a better quantization of FP32 than FP16 - just some loss in precision for gains in memory\n",
    "\n",
    "Google's hardware teams chose bfloat16 to improve hardware efficiency - being able to train accurate deep learning models, with minimal switching costs from FP32 (mixed precision training?). Apparently \"the physical size of a hardware multiplier scales with the square of the mantissa width\", so a smaller mantissa and trading off precision can lead to large gains in using less memory\n",
    "\n",
    "FAQs:\n",
    "1. Q: Don't we risk overflows when training with fp16 instead of fp32? Doesn't that crash training? Or are overflow values just capped?\n",
    "    - A: It just becomes NaNs when it overflows. The current framework is mixed precision training: 1) FP16 storage/memory for multiplications inputs - FP16 x FP16 -> FP32 (saves memory, speeds up compute), FP32 for accumulation matmul outputs (keeps sums stable), 3) model parameters are stored in FP32 and cast to FP16 for compute. It seems like bfloat16 allows you to just skip mixed-precision FP16 training and just use bfloat16 everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce786ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
