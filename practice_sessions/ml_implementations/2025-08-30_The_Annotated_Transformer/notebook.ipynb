{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc0e81c",
   "metadata": {},
   "source": [
    "# The Annotated Transformer\n",
    "\n",
    "**References**: \n",
    "- The Annotated Transformer, 2018 (https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "**Purpose**: \n",
    "- The Attention is All You Need paper is very clearly written, but it's quite difficult to implement correctly.\n",
    "- We want to improve translation quality of DL models as well as performance on NLP tasks.\n",
    "- The goal of reducing sequential computation leads us to CNNs which compute hidden representations in parallel for all input and output positions. \n",
    "- The number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. \n",
    "- Problem: This makes it more difficult to learn dependencies between distant positions.\n",
    "\n",
    "\n",
    "**Approach**: \n",
    "- The Annotated version of the paper has a line-by-line implementation.\n",
    "- In the Transformer, the number of operations required to relate signals from two arbitrary input or output positions is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect they counteract with Multi-Head Attention.\n",
    "\n",
    "**Result**: \n",
    "- Full-implementation which composes of 400 lines of library code which can process 27K tokens / sec on 4 GPUs.\n",
    "- Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution (enabling parallelized computation instead of sequential - which is critical when highly efficient training).\n",
    "\n",
    "\n",
    "\n",
    "**Definitions**: \n",
    "Self-attention/intra-attention = an attention mechanism relating different positions of a single sequence in order to compute a representaiton of the sequence\n",
    "\n",
    "**Notes**:\n",
    "- Self-attention had been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment andl earning task-indepeendent sentence representations. \n",
    "    - Note: it looks like self-attention was not a new idea, it had been proven already. The key insight was the scope of the problem because \"we need to learn dependencies between distant positions without needing the number of operations to relate signals to grow in distance between positions\" + what's SOTA in solving this problem right now (which is self-attention).\n",
    "    - In fact, E2E memory networks based on recurrent attention mechanism instead of sequence aligned recurrence have been shown to perform well on simple-language Q&A and language modeling tasks.\n",
    "        - Note: looks like the recurrent attention mechanism team didn't realize that they don't need the recurrent hidden state and can just process things in parallel instead of sequentially. This is where perspective and scope of the problem you care about really matters in helping you explore different options - the motivations are completely different.\n",
    "    - Another big takeaway is that typically you \"could\" just say that scale will win and eventually learn all of these things, but parallelization and higher efficiency was actually necessary here to realize those gains earlier on.\n",
    "\n",
    "**FAQs**:\n",
    "\n",
    "**Takeaways**:\n",
    "1. Note: looks like the recurrent attention mechanism team didn't realize that they don't need the recurrent hidden state and can just process things in parallel instead of sequentially. This is where perspective and scope of the problem you care about really matters in helping you explore different options - the motivations are completely different.\n",
    "2. Another big takeaway is that typically you \"could\" just say that scale will win and eventually learn all of these things, but parallelization and higher efficiency was actually necessary here to realize larger scale and those gains.\n",
    "\n",
    "**Action items**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ed5104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (3.10.6)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.18.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.17.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "     ---------------------------------------- 0.0/68.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 68.0/68.0 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: torch>=2.3.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torchtext) (2.8.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from seaborn) (2.3.2)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading charset_normalizer-3.4.3-cp311-cp311-win_amd64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch>=2.3.0->torchtext) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch>=2.3.0->torchtext) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch>=2.3.0->torchtext) (3.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch>=2.3.0->torchtext) (2025.7.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.3-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.3.0->torchtext) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.7-cp311-cp311-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.9/14.9 MB 27.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 3.0/14.9 MB 38.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.5/14.9 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.9/14.9 MB 50.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.4/14.9 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.7/14.9 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.9/14.9 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 46.9 MB/s eta 0:00:00\n",
      "Downloading torchtext-0.18.0-cp311-cp311-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.9/1.9 MB 129.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 41.3 MB/s eta 0:00:00\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/183.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 183.0/183.0 kB 10.8 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.13-cp311-cp311-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp311-cp311-win_amd64.whl (117 kB)\n",
      "   ---------------------------------------- 0.0/117.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 117.7/117.7 kB ? eta 0:00:00\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "   ---------------------------------------- 0.0/444.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 444.8/444.8 kB 14.0 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.4/2.0 MB 83.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 41.3 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.7/64.7 kB 3.4 MB/s eta 0:00:00\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 632.6/632.6 kB 19.4 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp311-cp311-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.8/1.8 MB 117.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 57.0 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.17.3-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.5/46.5 kB ? eta 0:00:00\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.3.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 2.7/6.2 MB 85.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.6/6.2 MB 58.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 66.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 49.8 MB/s eta 0:00:00\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "   ---------------------------------------- 0.0/161.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 161.2/161.2 kB 4.9 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.3-cp311-cp311-win_amd64.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 107.1/107.1 kB 6.5 MB/s eta 0:00:00\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "   ---------------------------------------- 0.0/102.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 102.2/102.2 kB ? eta 0:00:00\n",
      "Downloading cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 61.5/61.5 kB ? eta 0:00:00\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 76.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 85.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 49.3 MB/s eta 0:00:00\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "   ---------------------------------------- 0.0/243.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 243.4/243.4 kB ? eta 0:00:00\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 61.9/61.9 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "   ---------------------------------------- 0.0/129.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 129.8/129.8 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.3.1-cp311-cp311-win_amd64.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/144.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 144.0/144.0 kB 8.4 MB/s eta 0:00:00\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.3/87.3 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading wrapt-1.17.3-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-inspection, tqdm, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, idna, cloudpathlib, click, charset_normalizer, certifi, catalogue, blis, annotated-types, srsly, smart-open, requests, pydantic, preshed, markdown-it-py, language-data, torchtext, seaborn, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 certifi-2025.8.3 charset_normalizer-3.4.3 click-8.2.1 cloudpathlib-0.22.0 confection-0.1.5 cymem-2.0.11 idna-3.10 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 requests-2.32.5 rich-14.1.0 seaborn-0.13.2 shellingham-1.5.4 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 torchtext-0.18.0 tqdm-4.67.1 typer-0.17.3 typing-inspection-0.4.1 urllib3-2.5.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (2.8.0)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.23.0%2Bcpu-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.8.0%2Bcpu-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torchvision) (2.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kevihuang\\projects\\sketchbook\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchvision-0.23.0%2Bcpu-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 16.9 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.8.0%2Bcpu-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.5/2.5 MB 80.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 39.5 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.8.0+cpu torchvision-0.23.0+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext seaborn \n",
    "!pip install numpy matplotlib spacy torchtext seaborn \n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d668ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "898f9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many other models\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f55011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab) # logits (unnormalized logits for each token)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1) # log-probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0092b1",
   "metadata": {},
   "source": [
    "# Encoder and Decoder Stacks\n",
    "\n",
    "## Encoder\n",
    "The encoder is composed of a stack of N = 6 identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f17c4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\"Produce N identical layers.\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8d50d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Core encoder is a stack of N layers\"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Pass the input (and mask) through each layer in turn.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16b970be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Construct a layernorm module.\"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 # elementwise affine transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d15c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume d_model=512\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm. Pre-Layernorm\n",
    "    Note for code simplicity the norm is first as opposed to last. Also note: this is the classic method of a Pre-LayerNorm now which is empirically better than a Post-LayerNorm where\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.norm(x + self.dropout(sublayer(x)))\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d8aff",
   "metadata": {},
   "source": [
    "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62606f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # MHA\n",
    "        return self.sublayer[1](x, self.feed_forward) # FF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b83f1",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c33cc",
   "metadata": {},
   "source": [
    "The decoder is also composed of a stack of N = 6 identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d47c19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size) # d_model\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b13cb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5745316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent posiitons.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') # sets everything below the 1 upper triangle to 0\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f7d5453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2cfc5b6ea90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4177a3",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf6fec",
   "metadata": {},
   "source": [
    "Scaled Dot-Product Attention\n",
    "Input: queries and keys of dimension d_k, values of dimension d_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1f6e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # affinities / compatibility function\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1) # weights\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef29efe2",
   "metadata": {},
   "source": [
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. \n",
    "\n",
    "Employ h = 8 parallel attention layers/heads\n",
    "\n",
    "d_k = d_v = d_model / h = 64\n",
    "\n",
    "512 / 8 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bd39000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4) # Wq, Wk, Wv, Wo\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2038ef",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "Q: Do you need a mask for this FFN? Or I guess not? \n",
    "- A: No, the FFN just transforms one token embedding into another token embedding. That's it.\n",
    "\n",
    "Another way of describing this is as two convolutions with kernel size 1, projecting it from some input embedding dimension 1 to output embedding dimension 1, and then another one to project to output embeding dimension 2\n",
    "\n",
    "Input and output dimension = d_model = 512, inner-layer has d_ff = 2048 (512 * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a990b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388ebac",
   "metadata": {},
   "source": [
    "## Embeddings and Softmax\n",
    "norm = length / magnitude of a vector\n",
    "\n",
    "Euclidean norm (L2 norm) = ||x||_2 = \\sqrt(x_1^2 + x_2^2 + ... + x_n^2)\n",
    "\n",
    "Remember, Mean(X) = E[X] and Var(X) = E[(X - E[X])^2]\n",
    "\n",
    "When mean = 0, then Var(X) = E[X^2]\n",
    "\n",
    "Expected Squared Norm = \n",
    "\n",
    "E[||e_tok||^2] = E[x_1^2] + E[x_2^2] + ... + E[x_d_model^2]\n",
    "\n",
    "E[||e_tok||^2] = Var(x_1^2) + Var(x_2^2) + ... + Var(x_d_model^2)\n",
    "\n",
    "E[||e_tok||^2] = 1\n",
    "\n",
    "E[||e_tok||] = 1\n",
    "\n",
    "Positional encodings also all have magnitude 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44956164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model) # lookup table where rows = vocabulary items size vocab and cols = embedding dimensions d_model. Note: nn.Embedding by default initializes each dimenion to have variance = 1/d_model s.t. expected vector norm = 1.\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model) # scaling to not have positional encodings dominate # the key reason why we scale is because many sublayers like attention product outputs with variance = 1 per dimension. Therefore, to get the embedding's per-dimension variance to be variance = 1 instead of 1/d_model, we need to multiply by \\sqrt(self.d_model), assuming nn.Embedding initialized each dimension with variance 1/d_model so that the embedding vector norm = 1 # Therefore, we want all our vectors to have variance d_model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3d974",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Positional encodings can be learned or fixed, we'll use fixed sine and consine functions of different frequencies: \n",
    "\n",
    "PE_{(pos,2i)} = sin(pos/10000**{2i/d_model})\n",
    "\n",
    "PE_{(pos,2i+1)} = cos(pos/10000**{2i/d_model})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
