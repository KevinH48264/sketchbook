{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d769a465",
   "metadata": {},
   "source": [
    "# The First Law of Complexodynamics\n",
    "\n",
    "**Focus**: defining entropy with Kolmogorov complexity\n",
    "\n",
    "**References**: \n",
    "- The First Law of Complexodynamics (https://scottaaronson.blog/?p=762)\n",
    "\n",
    "**Purpose**: \"Why does 'complexity' or 'interestingness' of physical systems seem to increase with time and then hit a maximum and decrease, in contrast to the entropy, which of course increases monotonically?\"\n",
    "\n",
    "Challenges:\n",
    "1. Come up with a plausible formal definition of \"complexity\"\n",
    "2. Prove that the \"complexity\", so defined, is large at intermediate times in natural model systems, despite being close to zero at the initial time and close to zero at late times. \n",
    "\n",
    "- To spend more time in the math otherwise it's hard to follow the rules being set.\n",
    "\n",
    "**Approach**: Sketch a possible answer drawing on concepts from Kolmogorov complexity.\n",
    "- Even though isolated physical systems get monotonically more entropic, they *don't* get monotonically more \"complicated\" or \"interesting\".\n",
    "- Conjecture that the questions can be answered using a notion called sophistication from the theory of Kolmogorov complexity. \n",
    "\n",
    "\n",
    "**Definitions**: \n",
    "- Resource-Bounded Kolmogorov complexity: the length of the shortest computer program that outputs the state in a short amount of time. Usually O(t) because it'll eventually have to encode bits of each step, where 1 step = O(1) bits to encode, and t steps = O(t) bits to encode, because we do NOT allow the simulate forward program to take O(t) time (because it's resource-bounded / time bounded) but instead O(1) time.\n",
    "\n",
    "**Notes**:\n",
    "Problem 1: How to define entropy\n",
    "- Approach 1: Let's use Kolmogorov complexity\n",
    "\n",
    "- One subproblem is that expect entropy to increase at a linear or polynomial rate, but Kolmogorov complexity assuming a deterministic system increases entropy at a logarithmic rate with t (because it takes O(1) to encode the state and the deterministic function, and then O(log(t)) bits to encode time.)\n",
    "    - Q: Why is logarithmic growth a problem though?\n",
    "        - Assumption: In physics and thermodynamics, entropy is tied to the # of accessible microstates compatible with macroscopic constraints. So for many systems, that number grows exponentially with time or with system size - ie. entropy grows linearly or polynomially with time until equilibrium. The log(t) incerase suggests very slow disordering, which doesn't match physical reality.\n",
    "    - Approach 1.1: consider probabilistic systems rather than deterministic systems - Kolmogorov complexity really does increase at a polynomial rate (to encode all possible future states) instead of just time t. Specifically, if the system is probabilistic/truly random, then each step = O(1) bit, and so t steps = O(t) bits of randomness = polynomial rate.\n",
    "    - Approach 1.2: Replace Kolmogorov complexity by the resource-bounded Kolmogorov complexity: the length of the shortest computer program that outputs the state in a short amount of time. \n",
    "        - This forces us to look for a short description that is also fast, which generally don't exist (ie. the O(1) description we assumed doesn't always exist).\n",
    "        - Problem with resource-bounded Kolmogorov complexity - usually the simulate forward program takes O(t) time. Therefore, to output the state quickly (bounded) and not in O(t) time, the program can't simulate from scratch, so it must encode large chunks of the final state directly in its description --> now proportional to the length of the state itself - so the program must encode O(t) basically per step, which brings us back to Kolmogorov complexity under resource bounds = O(t), which matches physical entropy growth.\n",
    "\n",
    "**FAQs**:\n",
    "\n",
    "**Action items**:\n",
    "- Continue reading from \"OK, that was entropy.  What about the thing Sean was calling “complexity”...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d6f6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
