{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d769a465",
   "metadata": {},
   "source": [
    "# Optimizing ML Training with Metagradient Descent\n",
    "\n",
    "**Focus**: pass 1 and pass 2\n",
    "\n",
    "**References**: \n",
    "- Optimizing ML Training with Metagradient Descent (https://arxiv.org/pdf/2503.13751)\n",
    "\n",
    "**Purpose**: \n",
    "- Configuring the training process to maximize model performance is a major challenge in training large-scale ML models (like Neural Architecture Search but expanded?). Ex. finding the best training setup from a vast design space.\n",
    "\n",
    "**Approach**: \n",
    "- Take a gradient-based approach to this problem.\n",
    "- Introduce an algorithm for efficiently calculating metagradients - gradients through model training - at scale.\n",
    "- \"smooth mdoel training\" framework that enables effective optimization using metagradients\n",
    "\n",
    "**Result**: \n",
    "- with metagradient descent (MGD), they greatly improve existing dataset selection methods (?), outperform accuracy-degrading data poisoning attacks by an order of magnitude, and automatically find competitive learning rate schedules\n",
    "\n",
    "**Definitions**: \n",
    "- metagradient: gradient through model training?\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "\n",
    "**FAQs**:\n",
    "\n",
    "**Action items**:\n",
    "- Mainly just read the abstract so need to continue pass 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d6f6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
