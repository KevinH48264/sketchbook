{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d769a465",
   "metadata": {},
   "source": [
    "# Optimizing ML Training with Metagradient Descent\n",
    "\n",
    "**Focus**: pass 1 and pass 2\n",
    "\n",
    "**References**: \n",
    "- Optimizing ML Training with Metagradient Descent (https://arxiv.org/pdf/2503.13751)\n",
    "\n",
    "**Purpose**: \n",
    "- Configuring the training process to maximize model performance is a major challenge in training large-scale ML models (like Neural Architecture Search but expanded?). Ex. finding the best training setup from a vast design space.\n",
    "\n",
    "**Approach**: \n",
    "- Take a gradient-based approach to this problem.\n",
    "- Introduce an algorithm for efficiently calculating metagradients - gradients through model training - at scale.\n",
    "- \"smooth mdoel training\" framework that enables effective optimization using metagradients\n",
    "\n",
    "**Result**: \n",
    "- with metagradient descent (MGD), they greatly improve existing dataset selection methods (?), outperform accuracy-degrading data poisoning attacks by an order of magnitude, and automatically find competitive learning rate schedules\n",
    "\n",
    "**Definitions**: \n",
    "- metaparameters: training configuration / hyperparameters\n",
    "- metagradient: gradient through model training?\n",
    "\n",
    "**Notes**:\n",
    "- Deciding the data mix and architecture choice is challenging because there's a large design space. I wonder if they push some parameters to the extreme.\n",
    "- Note: even hyperparameter tuning is sometimes difficult\n",
    "\n",
    "Main approach is to take the optimization perspective on model training design. Kind of like NAS. \n",
    "- Deciding on a training configuration - a set of metaparameters - is a high-dimensional optimization problem (correct).\n",
    "\n",
    "Input space: all possible metaparameter choices (Q: how do you even know what this is? Does the human have to provide the input space? Probably right? Can't a model try to search for this? Is that where the key is?)\n",
    "- Ex: datapoints to train on, what model architecture to use, how to initalize model weights. - generally for these decisions, it looks like it's just one group that proves out that this is the configuration we should use basically. Then we just use that and mainly just tweak one thing - like only tweak the data. \n",
    "\n",
    "Objective function - takes in a set of parameters, trains a ML model according to those metaparameters, and then returns a target metric evaluated on that model (test accuracy). \n",
    "\n",
    "I think this is indeed a pretty good formulation, just need to check how to do this efficiently. Stopped at paragraph 2\n",
    "\n",
    "**FAQs**:\n",
    "1. I wonder if they push some parameters to the extreme to better understand how this parameter affects training? Or it's more gradient based?\n",
    "2. Can they re-create / re-discover the transformer architecture? What's going on here?\n",
    "3. It's a super big question - how can you possibly do better than a grid over hyperparameters? \n",
    "4. How is \"optimal\" training configuration defined? Probably for the absolute loss? Or rate of loss decrease?\n",
    "5. How does this compare to AlphaEvolve approach?\n",
    "6. The input space actually itself seems quite large. Shouldn't a model be trained to search and provide for that?\n",
    "7. How expensive is this training process? Do the scaling laws hold here where going through these steps on a smaller model and demonstrating gains at a small scale can translate to gains at a large scale?\n",
    "\n",
    "**Action items**:\n",
    "- Mainly just read the abstract so need to continue pass 1\n",
    "- The additional readings are going to be good as well: \n",
    "    - DataRater: Meta-Learned Dataset Curation (https://arxiv.org/pdf/2505.17895)\n",
    "    - MAGIC: Near-Optimal Data Attribution for Deep Learning (https://arxiv.org/pdf/2504.16430)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d6f6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
