{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5f963c",
   "metadata": {},
   "source": [
    "# AlphaEvolve/OpenEvolve\n",
    "\n",
    "**References**:\n",
    "- AlphaEvolve: A coding agent for scientific and\n",
    "algorithmic discovery (2025) (https://arxiv.org/pdf/2506.13131)\n",
    "- OpenEvolve: https://github.com/codelion/openevolve\n",
    "\n",
    "**Purpose**: Leverage LLMs for autonomous discovery without training\n",
    "\n",
    "**Approach**: Using a specialized scaffolding and context aware method for bootstrapping stronger and stronger solutions\n",
    "\n",
    "**Results**: AlphaEvolve discovered novel, provably correct algorithms that surpass SOTA solutions in math and computer science.\n",
    "\n",
    "**Definitions**: \n",
    "- AlphaEvolve = an evolutionary coding agent that substantially enhances capabilities of SOTA LLMs on highly challenging tasks such as tackling open scientific problems or optimizing critical pieces of computational infrastructure.\n",
    "\n",
    "**Notes**:\n",
    "- Primarily limited on time, will need to revisit this and likely the implementation itself.\n",
    "- Q: I wonder how this compares to math + coding RL vs scaffolding. Why do both lead to better solutions I think, and how do they compare and contrast? What's going on?\n",
    "- Q: I also wonder if there's a way to not have to maintain the database and kind of just have a memory system that just summarizes the best solutions so far as you go? Unclear. This is almost like a structured RAG, or a better version of RAG\n",
    "\n",
    "\n",
    "**Experiment: Evolving Two Sum**\n",
    "- Reference: https://gist.github.com/chunhualiao/571dd392121b6f7439cdab742063134c\n",
    "- Purpose: just testing OpenEvolve with the Leetcode Two Sum Python problem\n",
    "- Approach: naively leveraging OpenEvolve\n",
    "- Result: OpenEvolve declared that the O(n^2) brute-force approach was the best, which is incorrect O(n) is optimal.\n",
    "---\n",
    "- Purpose: to get OpenEvolve to discover the optimal O(n) solution after a few iterations\n",
    "- Approach: Digging into the logs suggested that the correctness and efficiency was weighted 70/30 by default, whereas Chunhua just set it to 50/50 to increase weighting for efficiency. There was also an expansion in the edge cases. And then full rewrite not the default diff-based evolution was implemented. \n",
    "- Result: OpenEvolve did discover the optimal O(n) solution\n",
    "- Takeaways: \n",
    "    1. designing the \"right\" reward function to elicit the desired solution and state is actually non-trivial. Specifically, the tests and the weightings heavily steer outcomes. Tests that are too easy will lead to suboptimal solutions, and less weighting on efficiency will also lead to suboptimal solutions because the agent can't tell the difference betwee na suboptimal and a \"more optimal solution\" because the combined score is 1 in both cases.\n",
    "    2. OpenEvolve takes 1-2 hours to get working and producing useful results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da74fcd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
